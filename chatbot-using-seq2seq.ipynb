{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7596308,"sourceType":"datasetVersion","datasetId":4421621}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from underthesea import word_tokenize\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nimport numpy as np\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:16:58.441136Z","iopub.execute_input":"2024-02-13T15:16:58.441438Z","iopub.status.idle":"2024-02-13T15:17:10.829429Z","shell.execute_reply.started":"2024-02-13T15:16:58.441410Z","shell.execute_reply":"2024-02-13T15:17:10.828647Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-13 15:17:00.093141: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-13 15:17:00.093237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-13 15:17:00.210029: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# đường dẫn tới file chứa dữ liệu câu hỏi và câu trả lời\ndata_path = '/kaggle/input/chat-simsimi-dataset/chatbotDataset.txt'\n\n# đọc dữ liệu từ file\nwith open(data_path, 'r', encoding='utf8') as f:\n    lines = f.read().split('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:17:10.831186Z","iopub.execute_input":"2024-02-13T15:17:10.832135Z","iopub.status.idle":"2024-02-13T15:17:10.884290Z","shell.execute_reply.started":"2024-02-13T15:17:10.832099Z","shell.execute_reply":"2024-02-13T15:17:10.883529Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"len(lines)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:17:10.885298Z","iopub.execute_input":"2024-02-13T15:17:10.885579Z","iopub.status.idle":"2024-02-13T15:17:10.892101Z","shell.execute_reply.started":"2024-02-13T15:17:10.885555Z","shell.execute_reply":"2024-02-13T15:17:10.891243Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"24783"},"metadata":{}}]},{"cell_type":"code","source":"Tokenizer = tf.keras.preprocessing.text.Tokenizer\npad_sequences = tf.keras.preprocessing.sequence.pad_sequences\nSequential = tf.keras.models.Sequential\nEmbedding = tf.keras.layers.Embedding\nSimpleRNN = tf.keras.layers.SimpleRNN\nDense = tf.keras.layers.Dense\nLSTM = tf.keras.layers.LSTM\nDropout = tf.keras.layers.Dropout","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:17:10.894111Z","iopub.execute_input":"2024-02-13T15:17:10.894370Z","iopub.status.idle":"2024-02-13T15:17:10.901519Z","shell.execute_reply.started":"2024-02-13T15:17:10.894347Z","shell.execute_reply":"2024-02-13T15:17:10.900711Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# tạo câu hỏi và câu trả lời dưới dạng chuỗi số\nquestion = []\nanswer = []\n\nfor line in lines:\n    if line:\n        line = line.strip()\n        parts = line.split(\":\")\n        if len(parts) == 2:\n          question.append(parts[0])\n          answer.append('<start> ' + parts[1] + ' <end>')","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:17:10.902528Z","iopub.execute_input":"2024-02-13T15:17:10.902841Z","iopub.status.idle":"2024-02-13T15:17:10.948166Z","shell.execute_reply.started":"2024-02-13T15:17:10.902811Z","shell.execute_reply":"2024-02-13T15:17:10.947473Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Tokenize the text\ntokenizer = Tokenizer(char_level=False, lower=True)\ntokenizer.fit_on_texts(question + answer)\n\nvocab_size = len(tokenizer.word_index) + 1\n\n# Convert text to sequences\nencoder_input =  pad_sequences(tokenizer.texts_to_sequences(question), padding='post')\ndecoder_input = pad_sequences(tokenizer.texts_to_sequences(answer), padding='post')\ndecoder_output = tf.keras.utils.to_categorical(pad_sequences([decode_ans[1:] for decode_ans in tokenizer.texts_to_sequences(answer)], maxlen=decoder_input.shape[1], padding='post'), vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:17:10.949246Z","iopub.execute_input":"2024-02-13T15:17:10.949589Z","iopub.status.idle":"2024-02-13T15:17:17.183760Z","shell.execute_reply.started":"2024-02-13T15:17:10.949559Z","shell.execute_reply":"2024-02-13T15:17:17.182934Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(\"encoder input shape:\", encoder_input.shape)\nprint(\"decoder input shape:\", decoder_input.shape)\nprint(\"encoder output shape:\", decoder_output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:17:17.185206Z","iopub.execute_input":"2024-02-13T15:17:17.185582Z","iopub.status.idle":"2024-02-13T15:17:17.190783Z","shell.execute_reply.started":"2024-02-13T15:17:17.185549Z","shell.execute_reply":"2024-02-13T15:17:17.189916Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"encoder input shape: (24775, 68)\ndecoder input shape: (24775, 70)\nencoder output shape: (24775, 70, 5021)\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_model():\n  hidden_units  = 258\n\n  encoder_inputs = tf.keras.layers.Input(shape=( encoder_input.shape[1] , ))\n  encoder_embedding = tf.keras.layers.Embedding( vocab_size, hidden_units , mask_zero=True ) (encoder_inputs)\n  encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( hidden_units , return_state=True )( encoder_embedding )\n  encoder_states = [ state_h , state_c ]\n\n  decoder_inputs = tf.keras.layers.Input(shape=( decoder_input.shape[1] ,  ))\n  decoder_embedding = tf.keras.layers.Embedding( vocab_size, hidden_units , mask_zero=True) (decoder_inputs)\n  decoder_lstm = tf.keras.layers.LSTM( hidden_units , return_state=True , return_sequences=True , name='decoder_lstm')\n  decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n\n  decoder_dense = tf.keras.layers.Dense( vocab_size , activation=tf.keras.activations.softmax )\n  output = decoder_dense ( decoder_outputs )\n\n  model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n  model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n\n  return model\n\nmodel = create_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:18:34.631538Z","iopub.execute_input":"2024-02-13T15:18:34.632283Z","iopub.status.idle":"2024-02-13T15:18:36.309316Z","shell.execute_reply.started":"2024-02-13T15:18:34.632254Z","shell.execute_reply":"2024-02-13T15:18:36.308432Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_3 (InputLayer)        [(None, 68)]                 0         []                            \n                                                                                                  \n input_4 (InputLayer)        [(None, 70)]                 0         []                            \n                                                                                                  \n embedding_2 (Embedding)     (None, 68, 258)              1295418   ['input_3[0][0]']             \n                                                                                                  \n embedding_3 (Embedding)     (None, 70, 258)              1295418   ['input_4[0][0]']             \n                                                                                                  \n lstm_1 (LSTM)               [(None, 258),                533544    ['embedding_2[0][0]']         \n                              (None, 258),                                                        \n                              (None, 258)]                                                        \n                                                                                                  \n decoder_lstm (LSTM)         [(None, 70, 258),            533544    ['embedding_3[0][0]',         \n                              (None, 258),                           'lstm_1[0][1]',              \n                              (None, 258)]                           'lstm_1[0][2]']              \n                                                                                                  \n dense_1 (Dense)             (None, 70, 5021)             1300439   ['decoder_lstm[0][0]']        \n                                                                                                  \n==================================================================================================\nTotal params: 4958363 (18.91 MB)\nTrainable params: 4958363 (18.91 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def data_generator(encoder_input, decoder_input, decoder_output, batch_size):\n    num_samples = len(encoder_input)\n    steps_per_epoch = num_samples // batch_size\n\n    while True:\n        for i in range(steps_per_epoch):\n            start = i * batch_size\n            end = (i + 1) * batch_size\n\n            encoder_input_batch = encoder_input[start:end]\n            decoder_input_batch = decoder_input[start:end]\n            decoder_output_batch = decoder_output[start:end]\n\n            yield ([encoder_input_batch, decoder_input_batch], decoder_output_batch)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:18:36.660238Z","iopub.execute_input":"2024-02-13T15:18:36.660974Z","iopub.status.idle":"2024-02-13T15:18:36.667002Z","shell.execute_reply.started":"2024-02-13T15:18:36.660943Z","shell.execute_reply":"2024-02-13T15:18:36.665972Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Train the model\nepochs = 1000  # Increase the number of epochs to give the model more time to learn\nbatch_size = 2048\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Định nghĩa callback để lưu mô hình có val_loss tốt nhất\ncheckpoint_filepath = 'best_model.h5df'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_best_only=True,\n    monitor='loss',\n    mode='min',\n    verbose=1\n)\n\n# train_val_split = int(len(encoder_input)*0.1)\n# train_data_generator = data_generator(encoder_input[train_val_split:], decoder_input[train_val_split:], decoder_output[train_val_split:], batch_size)\n# val_data_generator = data_generator(encoder_input[:train_val_split], decoder_input[:train_val_split], decoder_output[:train_val_split], batch_size)\n\n# history = model.fit_generator(generator=train_data_generator,\n#                               steps_per_epoch=len(encoder_input[train_val_split:]) // batch_size,\n#                               validation_data=val_data_generator,\n#                               validation_steps=len(encoder_input[:train_val_split]) // batch_size,\n#                               epochs=epochs,\n#                               callbacks=[model_checkpoint_callback]\n#                              )\n\ntrain_data_generator = data_generator(encoder_input, decoder_input, decoder_output, batch_size)\nhistory = model.fit_generator(generator=train_data_generator,\n                              steps_per_epoch=len(encoder_input) // batch_size,\n                              epochs=epochs,\n                              callbacks=[model_checkpoint_callback]\n                             )","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:18:44.642693Z","iopub.execute_input":"2024-02-13T15:18:44.643350Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/937938341.py:30: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  history = model.fit_generator(generator=train_data_generator,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1000\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1707837544.773915      97 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"12/12 [==============================] - ETA: 0s - loss: 8.4053\nEpoch 1: loss improved from inf to 8.40534, saving model to best_model.h5\n12/12 [==============================] - 58s 4s/step - loss: 8.4053\nEpoch 2/1000\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"12/12 [==============================] - ETA: 0s - loss: 6.2810\nEpoch 2: loss improved from 8.40534 to 6.28101, saving model to best_model.h5\n12/12 [==============================] - 39s 3s/step - loss: 6.2810\nEpoch 3/1000\n12/12 [==============================] - ETA: 0s - loss: 5.3488\nEpoch 3: loss improved from 6.28101 to 5.34876, saving model to best_model.h5\n12/12 [==============================] - 39s 3s/step - loss: 5.3488\nEpoch 4/1000\n12/12 [==============================] - ETA: 0s - loss: 5.2151\nEpoch 4: loss improved from 5.34876 to 5.21506, saving model to best_model.h5\n12/12 [==============================] - 39s 3s/step - loss: 5.2151\nEpoch 5/1000\n 6/12 [==============>...............] - ETA: 19s - loss: 5.1487","output_type":"stream"}]},{"cell_type":"code","source":"model = tf.keras.models.load_model('/kaggle/working/best_model.h5')\n\n# In thông tin về mô hình\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:17:22.012554Z","iopub.execute_input":"2024-02-13T15:17:22.012809Z","iopub.status.idle":"2024-02-13T15:17:23.047582Z","shell.execute_reply.started":"2024-02-13T15:17:22.012787Z","shell.execute_reply":"2024-02-13T15:17:23.046260Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/best_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# In thông tin về mô hình\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n","\u001b[0;31mOSError\u001b[0m: No file or directory found at /kaggle/working/best_model.h5"],"ename":"OSError","evalue":"No file or directory found at /kaggle/working/best_model.h5","output_type":"error"}]},{"cell_type":"code","source":"hidden_units = 1028\n\nencoder_inputs = model.input[0]\nencoder_states = model.layers[4].output[1:]\ndecoder_lstm = model.layers[5]\ndecoder_embedding = model.layers[3].output\ndecoder_dense = model.layers[6]\ndecoder_inputs = model.input[1]","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:17:23.048466Z","iopub.status.idle":"2024-02-13T15:17:23.048857Z","shell.execute_reply.started":"2024-02-13T15:17:23.048677Z","shell.execute_reply":"2024-02-13T15:17:23.048693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_inference_models():\n\n    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n\n    decoder_state_input_h = tf.keras.layers.Input(shape=( hidden_units ,))\n    decoder_state_input_c = tf.keras.layers.Input(shape=( hidden_units ,))\n\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n    decoder_outputs, state_h, state_c = decoder_lstm(\n        decoder_embedding , initial_state=decoder_states_inputs)\n\n    decoder_states = [state_h, state_c]\n\n    decoder_outputs = decoder_dense(decoder_outputs)\n\n    decoder_model = tf.keras.models.Model(\n        [decoder_inputs] + decoder_states_inputs,\n        [decoder_outputs] + decoder_states)\n\n    return encoder_model , decoder_model\n\ndef str_to_tokens( sentence : str ):\n    words = sentence.lower().split()\n    tokens_list = list()\n\n    for word in words:\n        tokens_list.append( tokenizer.word_index[ word ] )\n    return pad_sequences( [tokens_list] , maxlen=encoder_input.shape[1] , padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-02-12T16:15:38.537862Z","iopub.execute_input":"2024-02-12T16:15:38.538124Z","iopub.status.idle":"2024-02-12T16:15:38.545954Z","shell.execute_reply.started":"2024-02-12T16:15:38.538101Z","shell.execute_reply":"2024-02-12T16:15:38.544934Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"enc_model , dec_model = make_inference_models()\n\nfor _ in range(10):\n    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n    empty_target_seq = np.zeros( ( 1 , 1 ) )\n    empty_target_seq[0, 0] = tokenizer.word_index['start']\n    stop_condition = False\n    decoded_translation = ''\n    while not stop_condition :\n        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n        sampled_word = None\n        for word , index in tokenizer.word_index.items() :\n            if sampled_word_index == index :\n                decoded_translation += ' {}'.format( word )\n                sampled_word = word\n\n        if sampled_word == 'end' or len(decoded_translation.split()) >  decoder_input.shape[1]:\n            stop_condition = True\n\n        empty_target_seq = np.zeros( ( 1 , 1 ) )\n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        states_values = [ h , c ]\n\n    print( decoded_translation )","metadata":{"execution":{"iopub.status.busy":"2024-02-12T16:17:23.938666Z","iopub.execute_input":"2024-02-12T16:17:23.939481Z","iopub.status.idle":"2024-02-12T16:18:27.518913Z","shell.execute_reply.started":"2024-02-12T16:17:23.939450Z","shell.execute_reply":"2024-02-12T16:18:27.517637Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter question :  start\n"},{"name":"stdout","text":"1/1 [==============================] - 1s 1s/step\n1/1 [==============================] - 1s 1s/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n required parameter is not present end\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter question :  ngon\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n ngon vcl end\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter question :  chưa thấy ai ngon như mày\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n chó đẻ end\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter question :  biết ông huy không\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 19ms/step\n thằng chó điên end\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter question :  quá đã\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n thế nào end\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter question :  hề hề\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n kêu má a gãy hộ end\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter question :  ơ đcm\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n mày end\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter question :  iw\n"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m enc_model , dec_model \u001b[38;5;241m=\u001b[39m make_inference_models()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     states_values \u001b[38;5;241m=\u001b[39m enc_model\u001b[38;5;241m.\u001b[39mpredict( \u001b[43mstr_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnter question : \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m )\n\u001b[1;32m      5\u001b[0m     empty_target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros( ( \u001b[38;5;241m1\u001b[39m , \u001b[38;5;241m1\u001b[39m ) )\n\u001b[1;32m      6\u001b[0m     empty_target_seq[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","Cell \u001b[0;32mIn[12], line 28\u001b[0m, in \u001b[0;36mstr_to_tokens\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     25\u001b[0m tokens_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m---> 28\u001b[0m     tokens_list\u001b[38;5;241m.\u001b[39mappend( \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pad_sequences( [tokens_list] , maxlen\u001b[38;5;241m=\u001b[39mencoder_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] , padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mKeyError\u001b[0m: 'iw'"],"ename":"KeyError","evalue":"'iw'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}